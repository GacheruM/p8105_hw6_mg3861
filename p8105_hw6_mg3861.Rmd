---
title: "Homework 6"
author: "Margaret Gacheru"
date: "November 21, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(leaps)
```

#Problem 1

This dataset contains information on homicides in 50 large cities across the country.

```{r warning = FALSE, message = FALSE}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data = read_csv(url)

```


Tidy the dataset by creating a city_state variable and a binary variable indicating whether the homicide is solved and omitting cities Dallas, TX, Phoenix, AZ, Kansas City, MO, and Tulsa, AL. In addition, modify victim_race to have categories white and non-white (with white as the reference category) and convert victim_age to numeric. 

```{r warning = FALSE, message = FALSE}
tidy_homicide_data = 
  homicide_data%>%
  mutate(city_state = str_c(city, ", ", state))%>%
  select(city_state, everything())%>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))%>%
  mutate(victim_race = ifelse(victim_race == "White", "White", "Non-white"),
         victim_race = forcats::fct_relevel(victim_race, c("White", "Non-white")),
         victim_age = as.numeric(victim_age),
         crime_status = ifelse(disposition == "Closed by arrest", 1, 0))

```

For the city of Baltimore, MD, fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Then, obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed. 

```{r}

glm_output_md = tidy_homicide_data%>%
  filter(city_state == "Baltimore, MD")%>%
  glm(crime_status ~ victim_age + victim_sex + victim_race, family = binomial(link = "logit"), data = .)%>%
  broom::tidy()%>%
  mutate(conf.low = exp(estimate - 1.96*(std.error)),
         conf.high = exp(estimate + 1.96*(std.error)),
         estimate = exp(estimate))%>%
  select(term, estimate, conf.low, conf.high)%>%
  filter(term == "victim_raceNon-white")%>%
  mutate(term = str_replace(term, "victim_race", "Race: "))

glm_output_md%>%knitr::kable(col.names = c("Term", "Adjusted Odds Ratio", "Low CI", "High CI"))
  
```

Fit a logistic regression for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims

```{r}

glm_output = tidy_homicide_data%>%
  nest(uid:crime_status)%>%
  mutate(models = map(data, ~glm(crime_status ~ victim_age + victim_sex + victim_race, family = binomial(link = "logit"), data = .x)),
         models = map(models, broom::tidy))%>%
  select(-data)%>%
  unnest()%>%
  filter(term == "victim_raceNon-white")%>%
  mutate(conf.low = exp(estimate - 1.96*(std.error)),
         conf.high = exp(estimate + 1.96*(std.error)),
         estimate = exp(estimate))%>%
  select(city_state, estimate, conf.low, conf.high)

glm_output%>%knitr::kable(col.names = c("city_state", "Adjusted Odds Ratio (Non-white)", "Low CI", "High CI"))
  
```

Create a plot that shows the estimated ORs and CIs for each city and organize cities according to estimated OR.

```{r}

glm_output%>%
  ggplot(aes(reorder(city_state, estimate), estimate, color = city_state))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  labs(
    y = "Adjusted Odds Ratio",
    x = "City, State",
    title = "Odds Ratio for Solving Homicides Comparing Non-white to White Victims")+
  viridis::scale_color_viridis(discrete = TRUE)+
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90, hjust = 1))

```

For a majority of the cities, the estimate odds ratio for solving homicides for nonwhites in comparison to white victims is less than one. In addition, for about 50% of the cities, the odds ratio confidence interval does not include 1, indicating that the true odds for solving homicides for non-whites is significantly different from the odds for whites (in this case, the odds for nonwhites is lower). 


#Problem 2

Load and clean the data for regression analysis 
```{r}

birthweight_data = read_csv("./data/birthweight.csv")

birthweight_data = birthweight_data%>%
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

```

During the data cleaning process, one key observation is made:

1. The variables "pnumsga"" and "pnumlbw" have all entries equal to 0

```{r}

summary(birthweight_data$pnumsga)
summary(birthweight_data$pnumlbw)
```


As a starting point, we can fit the preliminary model based on factors that are believed to have an influence in low birth weights in the scientific world. Smoking is well known to have a negative impact on birth weights. Other factors that have been hypothesized to have an impact include baby's gender, parents' genes based on race, stress (income could be a potential replacement), number of prior low weight births, mother's age, size of parents, diabetes, high blood pressure, length of pregnancy, maternal's weight at the last week of gestation, and high risk pregnancy. We can use the variables that provide similar information.

```{r}

modified_birthweight_data = birthweight_data%>%
  mutate(babysex_male = ifelse(babysex == "1", "1", "0"),
         frace_black = ifelse(frace == "2", "1", "0"),
         mrace_black = ifelse(mrace == "2", "1", "0"))

fit = lm(bwt ~ bhead + blength + babysex_male + delwt  + frace_black + mrace_black + fincome + gaweeks + malform + mheight + momage + parity + ppbmi + smoken + wtgain, data = modified_birthweight_data)

summary(fit)

```


With an initial model, we can use criterion based procedures to determine the best model from the subset of variables 

```{r}

best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  


# Select the 'best' model of all subsets 
round(best(fit, nbest = 1),2)
```

Looking at the highest adjusted R-squared and lowest BIC & Cp, we are left with a model with baby's head circumference, baby's length at birth, baby's sex, mother's race (black or not), mother's weight at delivery, gestational age, mother's pre-pregnancy BMI, and average number of cigarettes smoked per day during pregnancy as the main effects.  

Show a plot of model residuals against fitted values 

```{r}

library(modelr)

fit = lm(bwt ~ bhead + blength + babysex_male + delwt + mrace_black + gaweeks + ppbmi + smoken, data = modified_birthweight_data)

modified_birthweight_data %>%
  gather_predictions(fit)%>%
  add_residuals(fit)%>%
  ggplot(aes(x = pred, y = resid))+
  geom_point()+
  labs( x= "Fitted Values",
        y = "Residuals")

```

The residuals vs. fitted values plot can be used to assess constant variance in the data. In this case, we observe a cluster around 0 towards the higher end of the fitted values. However, towards the lower end of the fitted values, we observe a slight curve and the residuals are not scattered around 0. This slightly violates constant variance assumption. 


Compare your model to two others: 

1. One using length at birth and gestational age as predictors (main effects only)

2. One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}

cv_df =
  crossv_mc(modified_birthweight_data, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(model1 = map(train, ~lm(bwt ~ bhead + blength + babysex_male + delwt + mrace_black + gaweeks + ppbmi + smoken, data = as_tibble(.x))),
         model2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model3 = map(train, ~lm(bwt ~ bhead + blength + babysex_male + bhead*blength + bhead*babysex_male + blength*babysex_male, data = .x))) %>% 
  mutate(rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))


cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()+
  labs(x = "Model",
       y = "RMSE",
       title = "Model Comparison")+
  scale_x_discrete(labels = c("Proposed", "Length_GestationalAge", "Interaction"))

```

Based on RMSE, my proposed model has a better predictive ability that the comparison models. The models with more predictors and complexity (interaction) performed better than the simple two-predictor model.  Among these three models, I would pick the proposed model for prediction purposes. However, in general, the proposed model might not be the optimal model, which was somewhat indicated by the residuals vs. fitted graph. 
